{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e661f9a-9291-4f32-973c-e52ac6254554",
   "metadata": {},
   "source": [
    "# RedisAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fd3c873-521a-4160-861c-4b78930ca2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from ml2rt import save_sklearn, load_model\n",
    "from redisai import Client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "277f6795-9a18-46af-83e4-4da766c1bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RedisAI Client\n",
    "redisai_client = Client(host='evc.re.kr', port=6379)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4c60943-3d90-4399-abb0-c9aa23fb656a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x08\\x08\\x12\\x08skl2onnx\\x1a\\x061.11.2\"\\x07ai.onnx(\\x002\\x00:\\xfa\\x04\\n\\x93\\x02\\n\\x08features\\x12\\x05label\\x12\\x12probability_tensor\\x1a\\x10LinearClassifier\"\\x10LinearClassifier*\\x1b\\n\\x10classlabels_ints@\\x00@\\x01@\\x02\\xa0\\x01\\x07*M\\n\\x0ccoefficients=#\\xd1\\xc4\\xbe=\\xaaAq?=\\xfa\\x17\\x17\\xc0=tf\\x80\\xbf=\\xf9\\x03\\xfe>=\\xed\\xeeh\\xbe=\\x0c\\xee5\\xbe=\\xce\\x1a5\\xbf=W\\xcb\\xe4\\xbd=\\xef\\x057\\xbf=\\xdbv\"@=\\xdb\\xf3\\xda?\\xa0\\x01\\x06*\\x1e\\n\\nintercepts=\\xe5\\xae\\rA=\\xb8-\\xc2?=\\x9c\\xf4%\\xc1\\xa0\\x01\\x06*\\x12\\n\\x0bmulti_class\\x18\\x01\\xa0\\x01\\x02*\\x1c\\n\\x0epost_transform\"\\x07SOFTMAX\\xa0\\x01\\x03:\\nai.onnx.ml\\n.\\n\\x05label\\x12\\x0coutput_label\\x1a\\x04Cast\"\\x04Cast*\\t\\n\\x02to\\x18\\x07\\xa0\\x01\\x02:\\x00\\nV\\n\\x12probability_tensor\\x12\\rprobabilities\\x1a\\nNormalizer\"\\nNormalizer*\\r\\n\\x04norm\"\\x02L1\\xa0\\x01\\x03:\\nai.onnx.ml\\n^\\n\\rprobabilities\\x12\\x12output_probability\\x1a\\x06ZipMap\"\\x06ZipMap*\\x1d\\n\\x12classlabels_int64s@\\x00@\\x01@\\x02\\xa0\\x01\\x07:\\nai.onnx.ml\\x12 c1fc8eb7409d4677b95523efcb907620Z\\x16\\n\\x08features\\x12\\n\\n\\x08\\x08\\x01\\x12\\x04\\n\\x02\\x08\\x04b\\x1a\\n\\x0coutput_label\\x12\\n\\n\\x08\\x08\\x07\\x12\\x04\\n\\x02\\x08\\x04b$\\n\\x12output_probability\\x12\\x0e\"\\x0c\\n\\n*\\x08\\x08\\x07\\x12\\x04\\n\\x02\\x08\\x01B\\x0e\\n\\nai.onnx.ml\\x10\\x01B\\x04\\n\\x00\\x10\\t'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model output path\n",
    "MODEL_OUTPUT_PATH = '../test_model/iris.onnx'\n",
    "model_onnx = load_model(MODEL_OUTPUT_PATH)\n",
    "model_onnx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fb24deaa-5c5a-46de-b137-e9b6062114f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 'iris-clf'' model is saved to RedisAI ðŸŸ¢.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3832452/331947039.py:1: DeprecationWarning: Call to deprecated method modelset. (Use modelstore instead) -- Deprecated since version 1.2.0.\n",
      "  redisai_client.modelset('iris-clf', 'onnx', 'cpu', model_onnx)\n"
     ]
    }
   ],
   "source": [
    "redisai_client.modelset('iris-clf', 'onnx', 'cpu', model_onnx)\n",
    "print(f\">> '{model_name}'' model is saved to RedisAI ðŸŸ¢.\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2cd445-4389-4e1c-b126-b5f2b8c1d6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b28177-323d-4ab5-99c6-9c9917b07b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc58d98-bcdd-4d8c-ab78-c03cb16d74f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e0d23-b8e5-4042-8b16-575f0502f41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7b686-5857-4d36-9872-151db21fa867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44c738fe-0271-47c9-83fc-625e444eea9d",
   "metadata": {},
   "source": [
    "- https://www.kaggle.com/code/pramod7/starter-iris-dataset-with-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fceec9f7-0123-4332-a325-6da5aa884c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n",
      "       'petal width (cm)', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import torch as tr\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as f\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "#load the iris data\n",
    "df=pd.read_csv('../input/iris.csv')\n",
    "# read the columns\n",
    "print (df.columns)\n",
    "\n",
    "# separate them into features and labels\n",
    "x=df.drop('target',axis=1).values\n",
    "y=df['target'].values\n",
    "\n",
    "# split them into train and test\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25)\n",
    "\n",
    "#convert x_train and x_test into float tensors.  y_train and y_test into int\n",
    "x_train=tr.FloatTensor(x_train)\n",
    "x_test=tr.FloatTensor(x_test)\n",
    "y_train=tr.LongTensor(y_train)\n",
    "y_test=tr.LongTensor(y_test)\n",
    "\n",
    "\n",
    "# build NN tensors model using python class objects, three hidden layers and fully connected by using linear model\n",
    "class model (nn.Module):\n",
    "    def __init__(self,in_feat=4,h1=12,h2=12,h3=9,out_feat=3):\n",
    "        super().__init__()\n",
    "        self.layr1=nn.Linear(in_feat,h1)\n",
    "        self.layr2=nn.Linear(h1,h2)\n",
    "        self.layr3=nn.Linear(h2,h3)\n",
    "        self.outlayr=nn.Linear(h3,out_feat)\n",
    "        \n",
    "    def forward(self,inpts):\n",
    "        out1=f.relu(self.layr1(inpts)) #output of 1st layer\n",
    "        out2=f.relu(self.layr2(out1))\n",
    "        out3=f.relu(self.layr3(out2))\n",
    "        outpt=self.outlayr(out3)\n",
    "        return outpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "15f8e2e4-1127-4de6-a2ab-3f2bb82572c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1  loss:     1.1844\n",
      "epoch:  6  loss:     1.0958\n",
      "epoch: 11  loss:     1.0634\n",
      "epoch: 16  loss:     0.7430\n",
      "epoch: 21  loss:     0.4722\n",
      "epoch: 26  loss:     0.4481\n",
      "epoch: 31  loss:     0.4032\n",
      "epoch: 36  loss:     0.3861\n",
      "epoch: 41  loss:     0.3503\n",
      "epoch: 46  loss:     0.2826\n",
      "epoch: 51  loss:     0.1843\n",
      "epoch: 56  loss:     0.1211\n",
      "epoch: 61  loss:     0.0775\n",
      "epoch: 66  loss:     0.0900\n",
      "epoch: 71  loss:     0.0736\n",
      "epoch: 76  loss:     0.0774\n",
      "epoch: 81  loss:     0.0768\n",
      "epoch: 86  loss:     0.0687\n",
      "epoch: 91  loss:     0.0617\n",
      "epoch: 96  loss:     0.0574\n",
      "epoch: 101  loss:     0.0565\n",
      "epoch: 106  loss:     0.0561\n",
      "epoch: 111  loss:     0.0550\n",
      "epoch: 116  loss:     0.0542\n",
      "epoch: 121  loss:     0.0537\n",
      "epoch: 126  loss:     0.0532\n",
      "epoch: 131  loss:     0.0528\n",
      "epoch: 136  loss:     0.0524\n",
      "epoch: 141  loss:     0.0520\n",
      "epoch: 146  loss:     0.0516\n"
     ]
    }
   ],
   "source": [
    "# data is ready, model is ready. Now lets prepare it to be trained with Epochs\n",
    "epochs=150\n",
    "loss_list=[]\n",
    "algo=model()\n",
    "\n",
    "#creating criterion for finding the loss values between predicted values and y train values\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = tr.optim.Adam(algo.parameters(), lr=0.1)\n",
    "\n",
    "for e in range(epochs):\n",
    "    e=e+1\n",
    "    training_pred=algo.forward(x_train)\n",
    "    loss=criterion(training_pred,y_train)\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    #below is to print the loss values for each epochs\n",
    "    if e%5==1:\n",
    "        print(f'epoch: {e:2}  loss: {loss.item():10.4f}')        \n",
    "    \n",
    "    #reset the optimizer    \n",
    "    optimizer.zero_grad() # reset grad\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "88b3c9e8-7ada-484c-8106-4936a9a09601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the loss list\n",
    "#plt.plot(range(epochs), loss_list)\n",
    "#plt.ylabel('Loss')\n",
    "#plt.xlabel('epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "59925a43-b795-4e39-ad32-5ffe16d45ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0319\n",
      " 1  tensor([-22.6962,  -0.9826,  11.3952]) 2\n",
      " 2  tensor([ 13.3147,  -0.6264, -22.8869]) 0\n",
      " 3  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      " 4  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      " 5  tensor([ 10.9170,   0.0352, -19.5683]) 0\n",
      " 6  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      " 7  tensor([-16.8521,   0.3071,   7.6936]) 2\n",
      " 8  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      " 9  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "10  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "11  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "12  tensor([-12.3881,   1.2922,   4.8662]) 2\n",
      "13  tensor([ 12.0256,  -0.2707, -21.1028]) 0\n",
      "14  tensor([-3.1092,  3.3398, -1.0110])    1\n",
      "15  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "16  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "17  tensor([-17.5459,   0.1540,   8.1330]) 2\n",
      "18  tensor([-20.9847,  -0.6049,  10.3112]) 2\n",
      "19  tensor([ 11.8498,  -0.2222, -20.8594]) 0\n",
      "20  tensor([ 10.6279,   0.1150, -19.1682]) 0\n",
      "21  tensor([-19.0413,  -0.1760,   9.0803]) 2\n",
      "22  tensor([ 13.3024,  -0.6231, -22.8700]) 0\n",
      "23  tensor([-17.7907,   0.0999,   8.2881]) 2\n",
      "24  tensor([-10.7951,   1.6437,   3.8572]) 2\n",
      "25  tensor([ 15.3485,  -1.1877, -25.7019]) 0\n",
      "26  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "27  tensor([-15.4202,   0.6231,   6.7867]) 2\n",
      "28  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "29  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "30  tensor([-21.5382,  -0.7270,  10.6618]) 2\n",
      "31  tensor([-7.0625,  2.4674,  1.4930])    1\n",
      "32  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "33  tensor([-13.7368,   0.9946,   5.7204]) 2\n",
      "34  tensor([ 10.3091,   0.2030, -18.7271]) 0\n",
      "35  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "36  tensor([ 11.8029,  -0.2093, -20.7946]) 0\n",
      "37  tensor([-7.9837,  2.2641,  2.0765])    1\n",
      "38  tensor([-1.9696,  3.5913, -1.7328])    1\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# we are testing the model to check the loss against test data, Here we do not require back propagation\n",
    "with tr.no_grad():\n",
    "    y_pred_loss=algo.forward(x_test)\n",
    "    val_loss=criterion(y_pred_loss, y_test)\n",
    "    print (f'{val_loss:.4f}')\n",
    "    \n",
    "#let us validate the predicated classes against the actual class\n",
    "correct=0\n",
    "with tr.no_grad():\n",
    "    for i,cls in enumerate (x_test):\n",
    "        y_pred=algo.forward(cls)\n",
    "        print(f'{i+1:2}  {str(y_pred):38} {y_test[i]}')\n",
    "        if y_pred.argmax().item()==y_test[i]:\n",
    "            correct=correct+1\n",
    "    print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9673065c-6a9e-4450-94f3-79997fb27053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03190468\n"
     ]
    }
   ],
   "source": [
    "from ml2rt import save_sklearn, load_model\n",
    "\n",
    "    \n",
    "fpath_model = './tmp/iris.pt'\n",
    "\n",
    "tr.save(algo.state_dict(), fpath_model)\n",
    "new_model = model()\n",
    "new_model.load_state_dict(tr.load(fpath_model))\n",
    "new_model.eval()\n",
    "\n",
    "with tr.no_grad():\n",
    "    y = new_model.forward(x_test)\n",
    "    loss = criterion(y, y_test)\n",
    "print(f'{loss:.8f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3a336297-6e6e-43f3-a375-34ec56b6e49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.6000, 3.0000, 6.6000, 2.1000],\n",
       "        [5.5000, 3.5000, 1.3000, 0.2000],\n",
       "        [5.7000, 2.9000, 4.2000, 1.3000],\n",
       "        [5.8000, 2.6000, 4.0000, 1.2000],\n",
       "        [4.9000, 3.0000, 1.4000, 0.2000],\n",
       "        [5.6000, 2.7000, 4.2000, 1.3000],\n",
       "        [6.7000, 3.0000, 5.2000, 2.3000],\n",
       "        [5.2000, 2.7000, 3.9000, 1.4000],\n",
       "        [5.1000, 2.5000, 3.0000, 1.1000],\n",
       "        [6.0000, 2.2000, 4.0000, 1.0000],\n",
       "        [5.7000, 2.8000, 4.1000, 1.3000],\n",
       "        [6.1000, 2.6000, 5.6000, 1.4000],\n",
       "        [5.1000, 3.5000, 1.4000, 0.3000],\n",
       "        [6.1000, 2.9000, 4.7000, 1.4000],\n",
       "        [6.7000, 3.1000, 4.7000, 1.5000],\n",
       "        [5.5000, 2.4000, 3.7000, 1.0000],\n",
       "        [5.7000, 2.5000, 5.0000, 2.0000],\n",
       "        [7.7000, 3.0000, 6.1000, 2.3000],\n",
       "        [5.0000, 3.3000, 1.4000, 0.2000],\n",
       "        [4.4000, 3.0000, 1.3000, 0.2000],\n",
       "        [6.9000, 3.2000, 5.7000, 2.3000],\n",
       "        [5.4000, 3.9000, 1.3000, 0.4000],\n",
       "        [7.4000, 2.8000, 6.1000, 1.9000],\n",
       "        [6.3000, 2.7000, 4.9000, 1.8000],\n",
       "        [5.8000, 4.0000, 1.2000, 0.2000],\n",
       "        [5.0000, 2.3000, 3.3000, 1.0000],\n",
       "        [5.8000, 2.7000, 5.1000, 1.9000],\n",
       "        [6.4000, 2.9000, 4.3000, 1.3000],\n",
       "        [5.8000, 2.7000, 3.9000, 1.2000],\n",
       "        [7.2000, 3.6000, 6.1000, 2.5000],\n",
       "        [6.7000, 3.0000, 5.0000, 1.7000],\n",
       "        [6.7000, 3.1000, 4.4000, 1.4000],\n",
       "        [7.2000, 3.2000, 6.0000, 1.8000],\n",
       "        [4.8000, 3.0000, 1.4000, 0.3000],\n",
       "        [6.2000, 2.9000, 4.3000, 1.3000],\n",
       "        [5.0000, 3.4000, 1.5000, 0.2000],\n",
       "        [6.3000, 2.5000, 4.9000, 1.5000],\n",
       "        [6.6000, 3.0000, 4.4000, 1.4000]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd804e-5a99-4c12-823f-a8db3aa8f1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb65dc0-6e8c-465f-9455-bc9ef9301f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "dl4cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
