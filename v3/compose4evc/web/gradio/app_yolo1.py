import gradio as gr
import time
from PIL import Image
import torch
from ultralytics import YOLO
import yaml
from yaml import load, dump
from yaml.loader import SafeLoader, BaseLoader, FullLoader, UnsafeLoader
import numpy as np
import cv2 
import onnxruntime as ort
import requests
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer

########################################################################################################## COCO Class List #####
coco_128_class_list = ['aeroplane', 'backpack', 'banana', 'baseball bat', 'baseball glove', #5
'bear', 'bed', 'bench', 'bicycle', 'bird', 
'boat', 'book', 'bottle', 'bowl', 'broccoli', 
'bus', 'cake', 'car', 'carrot', 'cat', 
'cell phone', 'chair', 'clock', 'cup', 
'diningtable','dog', 'donut', 'elephant', 'fork',
'frisbee', 'giraffe', 'handbag', 'horse', 'hot dog',
 'kite', 'knife', 'laptop', 'microwave', 'motorbike', 
 'mouse', 'orange', 'oven', 'person', 'pizza', 
 'pottedplant', 'refrigerator', 'remote', 'sandwich', 'scissors', 
 'sink', 'skateboard', 'skis', 'snowboard', 'sofa', 
 'spoon', 'sports ball', 'stop sign', 'suitcase', 'teddy bear', 
 'tennis racket', 'tie', 'toilet', 'toothbrush', 'traffic light', 
 'train', 'truck', 'tvmonitor', 'umbrella', 'vase',
 'wine glass', 'zebra']   # 71ê°œ 

########################################################################################################## Model ì •ì˜ #####
# ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©í•´ì„œ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. 
#GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# YOLO
model_yolo = YOLO("yolov8n.yaml")  # build a new model from scratch
model_yolo = YOLO('yolov8s.pt')
model_yolo.to(device)
# Image Captioning
model_vit = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
model_vit.to(device)

########################################################################################################## íŒŒë¼ë¯¸í„° ì •ì˜ #####
max_length = 16
num_beams = 4
gen_kwargs = {"max_length": max_length, "num_beams": num_beams}

model_path = 'yolov8s.pt' # yolo í›ˆë ¨ì‹œí‚¤ê¸° ì „ì— ì‚¬ì§„ ë„£ì—ˆì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ì—¬ ê¸°ë³¸ê°’ ì„¤ì • 
########################################################################################################## Function ì •ì˜ #####
# Function 1 : Image Caption
def predict_step(img_path):
    images = Image.open(img_path)
    
    pixel_values = feature_extractor(images=images, return_tensors="pt").pixel_values
    pixel_values = pixel_values.to(device)
    
    output_ids = model_vit.generate(pixel_values, **gen_kwargs)
    
    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    preds = [pred.strip() for pred in preds]
    global sentence
    sentence = preds[0]
    return sentence

# Function 2 : ì‚¬ìš©ìê°€ Textì…ë ¥í–ˆì„ ë•Œ Historyì— ì €ì¥í•˜ëŠ” ê³¼ì • 
def add_text(history, text):
    history = history + [(text, None)] # ì—¬ê¸°ì„œ textëŠ” user ì§ˆë¬¸
    print("addtext", history)
    return history, gr.update(value="", interactive=False) # ì–´ë–¤ í–‰ë™ì„ í–ˆì„ ë•Œ value=""ë¡œ ë°”ê¿”ë¼. # interactive?

# Function 3 : ì‚¬ìš©ìê°€ file uploadí–ˆì„ ë•Œ Historyì— ì €ì¥í•˜ëŠ” ê³¼ì • 
def add_file(history, file):
    history = history + [((file.name,),None)]
    print("addfile", history)
    return history

# Function 4 : ì‚¬ìš©ìì˜ ì…ë ¥ì— ë”°ë¼ YOLO trainì„ ì§„í–‰í•˜ê±°ë‚˜ ì˜¤ë¥˜ ë©”ì„¸ì§€ë¥¼ ë‚´ë³´ë‚´ëŠ” ê³¼ì • 
def bot(history):
    ### ì‚¬ìš©ìê°€ Textë¥¼ ì…ë ¥í–ˆì„ ë•Œ (=ì…ë ¥ì´ ì‚¬ì§„ì´ ì•„ë‹ ë•Œ) 
    if history[-1][0][0].lower().endswith(('.jpg', '.png', 'jpeg')) == False : # ì…ë ¥ì´ ì‚¬ì§„ì´ ì•„ë‹ ë•Œ 
        ### ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¨ ë‹¨ì–´ê°€ COCO Class ë¦¬ìŠ¤íŠ¸ì— ìˆë‹¤ë©´,
        if history[-1][0].lower() in coco_128_class_list:  
            input_class = history[-1][0]
            ### 1. yamlíŒŒì¼ ìˆ˜ì • 
            ### ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘” ë¹ˆ yamlíŒŒì¼ í™œìš© 
            with open('./empty.yaml', 'r') as f:
                data = yaml.load(f, Loader = SafeLoader)
                data['names'].append(input_class) 
                data['nc'] = len(data['names'])
    
            ### ìƒˆë¡œìš´ yamlíŒŒì¼ ìƒì„±
            with open(f'{input_class}.yaml', 'w') as g:
                print("yamlíŒŒì¼ data", data)
                yaml.dump(data, g, sort_keys=False, default_flow_style=False)
    
            ### YOLO í›ˆë ¨
            yaml_path = f'./{input_class}.yaml'
            model_yolo.train(data=yaml_path, epochs=3)
            
            global model_path
            model_path = model_yolo.export()  #ëª¨ë¸ì„ ì €ì¥í•˜ì 
            model_path = model_path.split('.')[0]+'.pt'
            print('ëª¨ë¸ê²½ë¡œ', model_path)
            
            response = "***Finished Training***"
            print('responseí•œ í›„',history)
            history[-1][1] = ""
            print('history[-1][1] = ""', history)
            for character in response:
                history[-1][1] += character
                time.sleep(0.05)
                yield history

        ### COCO listì— ì—†ëŠ” ë¬¸ìë¥¼ ì…ë ¥ ë°›ì•˜ì„ ë•Œ 
        else:
            response = "***sorry we can't train that object class***"
            print('responseí•œ í›„',history)
            history[-1][1] = ""
            print('history[-1][1] = ""', history)
            for character in response:
                history[-1][1] += character
                time.sleep(0.05)
                yield history 

    ### ì‚¬ìš©ìê°€ ì‚¬ì§„ì„ uploadí–ˆì„ ê²½ìš° 
    elif history[-1][0][0].lower().endswith(('.jpg', '.png', 'jpeg')) == True : # ì‚¬ì§„ì¼ ë•Œ 
        ### 1. YOLO
        global img_path
        img_path = history[-1][0][0]
        model_detect = YOLO(model_path)
        model_detect.to(device)
        results = model_detect(img_path)
        img = results[0].plot()
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Image.fromarrayë¥¼ ì“¸ ë•Œ output ì´ë¯¸ì§€ì˜ ìƒ‰ì´ ë°”ë€ŒëŠ” ê²ƒì„ ë°©ì§€ 
        img = Image.fromarray(img)
        img.save('yoloresult.jpg')
        response = ["yoloresult.jpg"]  # list ì•ˆì— ë‹´ì•„ì¤˜ì•¼í•¨ 
        history[-1][1] = response # history[-1][1] = bot ì‘ë‹µ ë„£ëŠ” ì¹¸ 
        print(history)
        yield history

### YOLO ê²°ê³¼ë¥¼ ì‘ë‹µìœ¼ë¡œ ë‚´ë³´ë‚¸ í›„, ë°”ë¡œ Image Captionì˜ ê²°ê³¼ë¥¼ ì‘ë‹µìœ¼ë¡œ ë‚´ë³´ëƒ„ 
def botbot(history):
    ### 2. Img Caption 
    predict_step(img_path)
    response2 = sentence 
    history = history + [[None, 'text']] 
    history[-1][1] = response2
    yield history
            
########################################################################################################## Chatbot í™”ë©´ êµ¬ì„± #####
with gr.Blocks() as demo:
    chatbot = gr.Chatbot( elem_id="chatbot").style(height=900) # ì±„íŒ…ë°© ë†’ì´ 

    with gr.Row():
        with gr.Column(scale=0.85):
            txt = gr.Textbox(
                show_label=False,
                placeholder="Enter text and press enter, or upload an image",scale=2).style(container=False)
        with gr.Column(scale=0.15, min_width=0):
            btn = gr.UploadButton("ğŸ“", file_types=["image", "video", "audio"])

    txt_msg = txt.submit(add_text, [chatbot, txt], [chatbot, txt], queue=True).then(
        bot, chatbot, chatbot
    )
    txt_msg.then(lambda: gr.update(interactive=True), None, [txt], queue=True)
    file_msg = btn.upload(add_file, [chatbot, btn], [chatbot], queue=True).then(
        bot,chatbot, chatbot
    )
    file_msg.then(botbot, chatbot, chatbot)


#demo.queue().launch(share=True, debug=True, inline=False)
demo.queue().launch(debug=True, 
            root_path="/chatbot",
            server_name="0.0.0.0")